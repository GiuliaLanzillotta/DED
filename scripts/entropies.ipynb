{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pub/hofmann-scratch/glanzillo/ded\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import socket\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "internal_path = os.path.abspath(os.path.join('.'))\n",
    "sys.path.append(internal_path)\n",
    "sys.path.append(internal_path + '/datasets')\n",
    "sys.path.append(internal_path + '/utils')\n",
    "\n",
    "import datetime\n",
    "import uuid\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import setproctitle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "\n",
    "\n",
    "import shutil\n",
    "from utils.args import add_management_args, add_rehearsal_args\n",
    "from utils.conf import set_random_seed, get_device, base_path\n",
    "from utils.status import ProgressBar\n",
    "from utils.stil_losses import *\n",
    "from utils.nets import *\n",
    "from torchvision.datasets import ImageNet, ImageFolder\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torchvision.models import efficientnet_v2_s, resnet50, ResNet50_Weights, googlenet, efficientnet_b0, mobilenet_v3_large\n",
    "from utils.eval import evaluate, validation_and_agreement, distance_models, validation_agreement_function_distance\n",
    "from dataset_utils.data_utils import load_dataset, CIFAR100sparse2coarse\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teacher entropy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum possible entropy = $$-log(1/C) = log(C)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(data_loader, model, device, C, temperature=1):\n",
    "    # running estimate of the outer products and mean\n",
    "    entropy = 0; labels_entropy=0; total=0\n",
    "    T = temperature\n",
    "    progress_bar = ProgressBar(verbose=True)\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        #if i==10: break # for testing\n",
    "        with torch.no_grad():\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                probabilities = torch.nn.functional.softmax(outputs/T, dim=1)\n",
    "                entropy += Categorical(probs = probabilities).entropy().sum()\n",
    "                labels_entropy += Categorical(probs=F.one_hot(labels, num_classes=C)).entropy().sum()\n",
    "                total += labels.shape[0]\n",
    "                \n",
    "        progress_bar.prog(i, len(data_loader), -1, 'Computing entropy', i/(len(data_loader)))  \n",
    "        \n",
    "    \n",
    "    return (entropy/total).detach().cpu().numpy(), (labels_entropy/total).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUID = 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(GPUID)\n",
    "device = get_device([GPUID])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMAGENET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum entropy = $6.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_root = \"/local/home/stuff/imagenet/\"\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "inference_transform = transforms.Compose([\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ])\n",
    "\n",
    "train_dataset = ImageFolder(imagenet_root+'train', inference_transform)\n",
    "all_indices = set(range(len(train_dataset)))\n",
    "random_indices = np.random.choice(list(all_indices), size=NUM_SAMPLES, replace=False)\n",
    "data = Subset(train_dataset, random_indices)\n",
    "loader = DataLoader(data, batch_size=32, shuffle=True, num_workers=4, pin_memory=False)\n",
    "\n",
    "# val_dataset = ImageFolder(imagenet_root+'val', inference_transform)\n",
    "# all_data = ConcatDataset([train_dataset, val_dataset])\n",
    "# all_data_loader = DataLoader(all_data, batch_size=64, shuffle=True, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(best=False, filename='checkpoint.pth.tar', student=False):\n",
    "    path = base_path() + \"chkpts\" + \"/\" + \"imagenet\" + \"/\" + \"resnet50/\"\n",
    "    if best: filepath = path + 'model_best.pth.tar'\n",
    "    else: filepath = path + filename\n",
    "    if os.path.exists(filepath):\n",
    "          print(f\"Loading existing checkpoint {filepath}\")\n",
    "          checkpoint = torch.load(filepath)  \n",
    "          if not student: \n",
    "              new_state_dict = {k.replace('module.','',1):v for (k,v) in checkpoint['state_dict'].items()}\n",
    "              checkpoint['state_dict'] = new_state_dict\n",
    "          return checkpoint\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing checkpoint ./logs/chkpts/imagenet/resnet50/rn50_2023-02-21_10-45-30_best.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialising the model\n",
    "teacher =  resnet50(weights=None)\n",
    "\n",
    "CHKPT_NAME = 'rn50_2023-02-21_10-45-30_best.ckpt'\n",
    "checkpoint = load_checkpoint(best=False, filename=CHKPT_NAME, student=False) \n",
    "if checkpoint: \n",
    "        teacher.load_state_dict(checkpoint['state_dict'])\n",
    "        teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 11-28 | 11:52 ] Task Computing entropy | epoch -1: |██████████████████████████████████████████████████| 5.6 ep/h | loss: 0.999936 ||"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.88050705, dtype=float32), array(1.192093e-07, dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_entropy, label_entropy = compute_entropy(loader, teacher, C=1000, device=device)\n",
    "teacher_entropy, label_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 5M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximim entropy = $2.3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR 5mil...\n",
      "Loaded part 1/6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m C5m_train, C5m_test \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39mcifar5m\u001b[39;49m\u001b[39m'\u001b[39;49m, augment\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/pub/hofmann-scratch/glanzillo/ded/dataset_utils/data_utils.py:135\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(name, augment)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m load_cifar100(augment)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcifar5m\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m load_cifar5m(augment)\n\u001b[1;32m    138\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m/pub/hofmann-scratch/glanzillo/ded/dataset_utils/data_utils.py:91\u001b[0m, in \u001b[0;36mload_cifar5m\u001b[0;34m(augment)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39m#cifar5m_root = \"/local/home/stuff/cifar-5m/\"\u001b[39;00m\n\u001b[1;32m     90\u001b[0m data \u001b[39m=\u001b[39m Cifar5MData(root\u001b[39m=\u001b[39mcifar5m_root)\n\u001b[0;32m---> 91\u001b[0m data\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     92\u001b[0m train_dataset \u001b[39m=\u001b[39m Cifar5M(cifar5m_root, data, train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, augmentations\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mCompose(augmentations))\n\u001b[1;32m     93\u001b[0m val_dataset \u001b[39m=\u001b[39m Cifar5M(cifar5m_root, data, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/pub/hofmann-scratch/glanzillo/ded/dataset_utils/cifar5m.py:34\u001b[0m, in \u001b[0;36mCifar5MData.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     33\u001b[0m     z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcifar5m-part\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.npz\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> 34\u001b[0m     X_tr[i\u001b[39m*\u001b[39mnpart: (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mnpart] \u001b[39m=\u001b[39m z[\u001b[39m'\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     35\u001b[0m     Ys\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mtensor(z[\u001b[39m'\u001b[39m\u001b[39mY\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mlong())\n\u001b[1;32m     36\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoaded part \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/6\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/npyio.py:245\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m magic \u001b[39m==\u001b[39m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    244\u001b[0m     \u001b[39mbytes\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mopen(key)\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(\u001b[39mbytes\u001b[39;49m,\n\u001b[1;32m    246\u001b[0m                              allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallow_pickle,\n\u001b[1;32m    247\u001b[0m                              pickle_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpickle_kwargs)\n\u001b[1;32m    248\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mzip\u001b[39m.\u001b[39mread(key)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:777\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m             read_count \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(max_read_count, count \u001b[39m-\u001b[39m i)\n\u001b[1;32m    776\u001b[0m             read_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(read_count \u001b[39m*\u001b[39m dtype\u001b[39m.\u001b[39mitemsize)\n\u001b[0;32m--> 777\u001b[0m             data \u001b[39m=\u001b[39m _read_bytes(fp, read_size, \u001b[39m\"\u001b[39;49m\u001b[39marray data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    778\u001b[0m             array[i:i\u001b[39m+\u001b[39mread_count] \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mfrombuffer(data, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    779\u001b[0m                                                      count\u001b[39m=\u001b[39mread_count)\n\u001b[1;32m    781\u001b[0m \u001b[39mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/lib/format.py:906\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     \u001b[39m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[39m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    904\u001b[0m     \u001b[39m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    905\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m         r \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mread(size \u001b[39m-\u001b[39;49m \u001b[39mlen\u001b[39;49m(data))\n\u001b[1;32m    907\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m r\n\u001b[1;32m    908\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(r) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m==\u001b[39m size:\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:927\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_offset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    926\u001b[0m \u001b[39mwhile\u001b[39;00m n \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof:\n\u001b[0;32m--> 927\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read1(n)\n\u001b[1;32m    928\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(data):\n\u001b[1;32m    929\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_readbuffer \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:1017\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_left \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1016\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_crc(data)\n\u001b[1;32m   1018\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/lib/python3.10/zipfile.py:942\u001b[0m, in \u001b[0;36mZipExtFile._update_crc\u001b[0;34m(self, newdata)\u001b[0m\n\u001b[1;32m    939\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    940\u001b[0m     \u001b[39m# No need to compute the CRC if we don't have a reference value\u001b[39;00m\n\u001b[1;32m    941\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 942\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m=\u001b[39m crc32(newdata, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_running_crc)\n\u001b[1;32m    943\u001b[0m \u001b[39m# Check the CRC if we're at the end of the file\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eof \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running_crc \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expected_crc:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "C5m_train, C5m_test = load_dataset('cifar5m', augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly drawing 500000 samples for the Cifar5M base\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Randomly drawing {NUM_SAMPLES} samples for the Cifar5M base\")\n",
    "all_indices = set(range(len(C5m_train)))\n",
    "random_indices = np.random.choice(list(all_indices), size=NUM_SAMPLES, replace=False)\n",
    "data = Subset(C5m_train, random_indices)\n",
    "loader = DataLoader(data, batch_size=128, shuffle=True, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(best=False, filename='checkpoint.pth.tar', type='mnet'):\n",
    "    \"\"\" Available network types: [mnet, convnet]\"\"\"\n",
    "    path = base_path() + \"chkpts\" + \"/\" + \"cifar5m\" + \"/\" + f\"{type}/\"\n",
    "    if best: filepath = path + 'model_best.ckpt'\n",
    "    else: filepath = path + filename\n",
    "    if os.path.exists(filepath):\n",
    "          print(f\"Loading existing checkpoint {filepath}\")\n",
    "          checkpoint = torch.load(filepath)\n",
    "          return checkpoint\n",
    "    return None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTROPY = 0.35\n",
    "teacher = mobilenet_v3_large(num_classes=10) # adjusting for CIFAR \n",
    "CHKPT_NAME = f'mnet-teacher.ckpt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN made with 154250 parameters\n"
     ]
    }
   ],
   "source": [
    "# ENTROPY = 2.26\n",
    "teacher = make_cnn(c=20, num_classes=10, use_batch_norm=True)\n",
    "CHKPT_NAME = f'convnet-teacher.ckpt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing checkpoint ./logs/chkpts/cifar5m/mnet/mnet-teacher.ckpt\n"
     ]
    }
   ],
   "source": [
    "checkpoint = load_checkpoint(best=False, filename=CHKPT_NAME, type='mnet') \n",
    "if checkpoint: \n",
    "        teacher.load_state_dict(checkpoint['state_dict'])\n",
    "        teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 11-28 | 11:28 ] Task Computing entropy | epoch -1: |██████████████████████████████████████████████████| 101.68 ep/h | loss: 0.99974405 |"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.355543, dtype=float32), array(1.192093e-07, dtype=float32))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_entropy, label_entropy = compute_entropy(loader, teacher, C=10, device=device)\n",
    "teacher_entropy, label_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maximum entropy = $4.6$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "C100_train, C100_val = load_dataset('cifar100', augment=False)\n",
    "all_data = ConcatDataset([C100_train, C100_val])\n",
    "loader = DataLoader(all_data, batch_size=128, shuffle=True, num_workers=4, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(best=False, filename='checkpoint.pth.tar', distributed=False):\n",
    "    path = base_path() + \"chkpts\" + \"/\" + \"cifar100\" + \"/\" + \"resnet18/\"\n",
    "    if best: filepath = path + 'model_best.ckpt'\n",
    "    else: filepath = path + filename\n",
    "    if os.path.exists(filepath):\n",
    "          print(f\"Loading existing checkpoint {filepath}\")\n",
    "          checkpoint = torch.load(filepath)\n",
    "          return checkpoint\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing checkpoint ./logs/chkpts/cifar100/resnet18/resnet18-teacher.ckpt\n"
     ]
    }
   ],
   "source": [
    "# initialising the model\n",
    "teacher = resnet18(num_classes=100)\n",
    "#teacher = make_cnn(c=150, num_classes=100, use_batch_norm=True)\n",
    "CHKPT_NAME = 'resnet18-teacher.ckpt'\n",
    "#CHKPT_NAME = 'convnet150-teacher.ckpt'\n",
    "checkpoint = load_checkpoint(best=False, filename=CHKPT_NAME) \n",
    "if checkpoint: \n",
    "        teacher.load_state_dict(checkpoint['state_dict'])\n",
    "        teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 11-28 | 11:30 ] Task Computing entropy | epoch -1: |██████████████████████████████████████████████████| 735.67 ep/h | loss: 0.9978678 ||"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.44312614, dtype=float32), array(1.192093e-07, dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_entropy, label_entropy = compute_entropy(loader, teacher, C=100, device=device)\n",
    "teacher_entropy, label_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 11-28 | 11:54 ] Task Computing entropy | epoch -1: |██████████████████████████████████████████████████| 751.55 ep/h | loss: 0.9978678 ||"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(4.5765233, dtype=float32), array(1.192093e-07, dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_entropy, label_entropy = compute_entropy(loader, teacher, C=100, device=device, temperature=10)\n",
    "teacher_entropy, label_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 11-28 | 11:54 ] Task Computing entropy | epoch -1: |██████████████████████████████████████████████████| 743.29 ep/h | loss: 0.9978678 ||"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(0.02093775, dtype=float32), array(1.192093e-07, dtype=float32))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_entropy, label_entropy = compute_entropy(loader, teacher, C=100, device=device, temperature=0.1)\n",
    "teacher_entropy, label_entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
